{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AdamW\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]\n",
    "\n",
    "token = BertTokenizer.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    sents = [data[i][0] for i in range(len(data))]\n",
    "    labels = [data[i][1] for i in range(len(data))]\n",
    "    \n",
    "    data = token.batch_encode_plus(batch_text_or_text_pairs=sents,\n",
    "                    truncation=True, \n",
    "                    padding='max_length',\n",
    "                    max_length=50,\n",
    "                    return_tensors='pt',\n",
    "                    return_length=True)\n",
    "    \n",
    "    input_ids = data['input_ids'] \n",
    "    attention_mask = data['attention_mask'] \n",
    "    token_type_ids = data['token_type_ids'] \n",
    "    labels = torch.LongTensor(labels) \n",
    "    \n",
    "    return input_ids, attention_mask, token_type_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('2022.5.4_5_model.pth',map_location ='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) 喜\n"
     ]
    }
   ],
   "source": [
    "test_dataset = Dataset([\"陳伯彥跟鬼一樣\"], [7414]) #0是喜 1是怒 2是哀\n",
    "testloader = DataLoader(dataset=test_dataset, batch_size=1, collate_fn=collate_fn)\n",
    "for i in testloader:\n",
    "    tokens_tensors, segments_tensors, masks_tensors = i[:3]\n",
    "    outputs = model(input_ids=tokens_tensors, \n",
    "          token_type_ids=segments_tensors, \n",
    "          attention_mask=masks_tensors)\n",
    "    logits = outputs[0]\n",
    "    labels = i[3]\n",
    "    _, pred = torch.max(logits.data, 1)\n",
    "    if pred[0] == 0:\n",
    "        print(pred[0], '喜')\n",
    "    elif pred[0] == 1:\n",
    "        print(pred[0], '怒')\n",
    "    elif pred[0] == 2:\n",
    "        print(pred[0], '哀')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
